#!/usr/bin/env python3
"""
Wisdom Splitter - å‚ç…§é »åº¦åŸºç›¤ Wisdom è‡ªå‹•åˆ†å‰²

æ ¹æœ¬çš„è§£æ±ºç­–:
- ã‚ˆãå‚ç…§ã•ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ (core) vs ãŸã¾ã«å‚ç…§ã•ã‚Œã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³ (feature) åˆ†é›¢
- ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‚ãŸã‚Šã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚³ã‚¹ãƒˆæœ€é©åŒ– (coreã®ã¿è‡ªå‹•ãƒ­ãƒ¼ãƒ‰)
- æ‹¡å¼µæ€§ç¢ºä¿ (150æ©Ÿèƒ½ã¾ã§å¯¾å¿œ)
"""

import json
import re
from pathlib import Path
from typing import Dict, List, Tuple
from dataclasses import dataclass

# wisdom_ttl_trackerã‚’import
import sys
sys.path.insert(0, str(Path(__file__).parent))
from wisdom_ttl_tracker import WisdomTTLTracker, SectionMetadata


@dataclass
class SectionContent:
    """ã‚»ã‚¯ã‚·ãƒ§ãƒ³ å†…å®¹"""
    title: str
    content: str
    metadata: SectionMetadata


class WisdomSplitter:
    """Wisdom è‡ªå‹•åˆ†å‰² æ™‚ã‚¹ãƒ†ãƒ """

    def __init__(self, wisdom_dir: Path = None):
        self.wisdom_dir = wisdom_dir or Path(".claude/wisdom")
        self.tracker = WisdomTTLTracker(wisdom_dir)
        self.core_threshold = 5  # 5å›ä»¥ä¸Šå‚ç…§ â†’ core

    def split_patterns(self, dry_run: bool = False) -> Dict:
        """project-patterns.mdã‚’core/featureã§åˆ†å‰²"""
        source_file = self.wisdom_dir / "project-patterns.md"
        if not source_file.exists():
            raise FileNotFoundError(f"{source_file} ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")

        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ æŠ½å‡º
        sections = self._extract_sections_with_content(source_file)

        # core/feature åˆ†é¡
        core_sections = []
        feature_sections = []

        for section in sections:
            if section.metadata.reference_count >= self.core_threshold:
                core_sections.append(section)
            else:
                feature_sections.append(section)

        # çµ±è¨ˆ
        result = {
            "source_file": source_file.name,
            "total_sections": len(sections),
            "core_sections": len(core_sections),
            "feature_sections": len(feature_sections),
            "core_threshold": self.core_threshold
        }

        if not dry_run:
            # ãƒ•ã‚¡ã‚¤ãƒ« ç”Ÿæˆ
            self._write_split_file("core-patterns.md", core_sections, is_core=True)
            self._write_split_file("feature-patterns.md", feature_sections, is_core=False)

            # åŸæœ¬ ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
            backup_file = self.wisdom_dir / f"{source_file.name}.backup"
            source_file.rename(backup_file)
            result["backup_file"] = backup_file.name

        return result

    def merge_patterns(self) -> None:
        """core/featureã‚’å†ã³ project-patterns.mdã§ãƒãƒ¼ã‚¸"""
        core_file = self.wisdom_dir / "core-patterns.md"
        feature_file = self.wisdom_dir / "feature-patterns.md"

        if not (core_file.exists() and feature_file.exists()):
            raise FileNotFoundError("core-patterns.md ã¾ãŸã¯ã¯ feature-patterns.mdãŒã‚ã‚Šã¾ã›ã‚“")

        # ãƒãƒ¼ã‚¸
        merged_content = "# Project Patterns\n\n"
        merged_content += "> ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ core-patterns.mdã¨feature-patterns.mdã‚’ãƒãƒ¼ã‚¸ã—ãŸ ã‚‚ã®ã§ã™ã€‚\n\n"
        merged_content += "---\n\n"

        merged_content += "## Core Patterns (ã‚ˆã å‚ç…§)\n\n"
        merged_content += core_file.read_text(encoding="utf-8").split("\n", 3)[-1]
        merged_content += "\n\n---\n\n"

        merged_content += "## Feature Patterns (ãŸã¾ã« å‚ç…§)\n\n"
        merged_content += feature_file.read_text(encoding="utf-8").split("\n", 3)[-1]

        output_file = self.wisdom_dir / "project-patterns.md"
        output_file.write_text(merged_content, encoding="utf-8")

        print(f"âœ… ãƒãƒ¼ã‚¸ å®Œäº†: {output_file}")

    def _extract_sections_with_content(self, file_path: Path) -> List[SectionContent]:
        """ã‚»ã‚¯ã‚·ãƒ§ãƒ³åˆ¥å†…å®¹æŠ½å‡º"""
        content = file_path.read_text(encoding="utf-8")
        sections = []

        # ## ãƒ¬ãƒ™ãƒ« ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§åˆ†å‰²
        pattern = r'^##\s+(.+?)$\n(.*?)(?=^##\s+|\Z)'
        matches = re.finditer(pattern, content, re.MULTILINE | re.DOTALL)

        for match in matches:
            title = match.group(1).strip()
            section_content = match.group(2).strip()

            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ç…§ä¼š
            section_id = self.tracker._normalize_section_id(title)
            all_metadata = self.tracker.get_sections_by_file(file_path.name)
            metadata = next(
                (m for m in all_metadata if m.section_id == section_id),
                None
            )

            if metadata:
                sections.append(SectionContent(
                    title=title,
                    content=section_content,
                    metadata=metadata
                ))

        return sections

    def _write_split_file(
        self,
        filename: str,
        sections: List[SectionContent],
        is_core: bool
    ) -> None:
        """åˆ†å‰²ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
        output_file = self.wisdom_dir / filename

        # ãƒ˜ãƒƒãƒ€ãƒ¼
        content = f"# {filename.replace('-', ' ').replace('.md', '').title()}\n\n"

        if is_core:
            content += "> **ã‚ˆãå‚ç…§ã•ã‚Œã‚‹æ ¸å¿ƒãƒ‘ã‚¿ãƒ¼ãƒ³**\n"
            content += "> ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã¯ å…¨ã¦ã® ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§è‡ªå‹•çš„ã« ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ã€‚\n\n"
        else:
            content += "> **ãŸã¾ã«å‚ç…§ã•ã‚Œã‚‹æ©Ÿèƒ½åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³**\n"
            content += "> å¿…è¦ãªæ™‚ã®ã¿æ˜ç¤ºçš„ã«å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n\n"

        content += f"åˆè¨ˆ {len(sections)}å€‹ ã‚»ã‚¯ã‚·ãƒ§ãƒ³ (å‚ç…§ é–¾å€¤: {self.core_threshold}å›)\n\n"
        content += "---\n\n"

        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³ è¿½åŠ 
        for section in sections:
            content += f"## {section.title}\n\n"
            content += section.content
            content += f"\n\n<!-- å‚ç…§: {section.metadata.reference_count}å› -->\n\n"

        output_file.write_text(content, encoding="utf-8")
        print(f"âœ… ç”Ÿæˆ: {filename} ({len(sections)} ã‚»ã‚¯ã‚·ãƒ§ãƒ³)")

    def analyze_split_impact(self) -> Dict:
        """åˆ†å‰² å½±éŸ¿ åˆ†æ (ãƒˆãƒ¼ã‚¯ãƒ³ ã‚³ã‚¹ãƒˆ äºˆæ¸¬)"""
        source_file = self.wisdom_dir / "project-patterns.md"
        if not source_file.exists():
            raise FileNotFoundError(f"{source_file} ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚Šã¾ã›ã‚“")

        sections = self._extract_sections_with_content(source_file)

        core_size = 0
        feature_size = 0

        for section in sections:
            section_size = len(section.title) + len(section.content)
            if section.metadata.reference_count >= self.core_threshold:
                core_size += section_size
            else:
                feature_size += section_size

        total_size = core_size + feature_size

        return {
            "total_size_bytes": total_size,
            "core_size_bytes": core_size,
            "feature_size_bytes": feature_size,
            "core_percentage": (core_size / total_size * 100) if total_size else 0,
            "estimated_tokens_before": total_size // 4,  # 1 token â‰ˆ 4 bytes
            "estimated_tokens_after": core_size // 4,  # featureã¯ãƒ­ãƒ¼ãƒ‰ã—ãªã„
            "token_savings": (feature_size // 4),
            "savings_percentage": (feature_size / total_size * 100) if total_size else 0
        }


def main():
    """CLI ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ"""
    import argparse

    parser = argparse.ArgumentParser(
        description="Wisdom Splitter - å‚ç…§é »åº¦åŸºç›¤ è‡ªå‹•åˆ†å‰²"
    )
    parser.add_argument(
        "--split",
        action="store_true",
        help="project-patterns.mdã‚’core/featureã§åˆ†å‰²"
    )
    parser.add_argument(
        "--merge",
        action="store_true",
        help="core/featureã‚’å†ã³ ãƒãƒ¼ã‚¸"
    )
    parser.add_argument(
        "--analyze",
        action="store_true",
        help="åˆ†å‰² å½±éŸ¿ åˆ†æ (ãƒˆãƒ¼ã‚¯ãƒ³ å‰Šæ¸› äºˆæ¸¬)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆãªã—ã§ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã®ã¿"
    )
    parser.add_argument(
        "--threshold",
        type=int,
        default=5,
        help="core é–¾å€¤ (åŸºæœ¬: 5å›)"
    )

    args = parser.parse_args()
    splitter = WisdomSplitter()
    splitter.core_threshold = args.threshold

    if args.analyze:
        print("\nğŸ“Š åˆ†å‰² å½±éŸ¿ åˆ†æ")
        print("=" * 60)
        impact = splitter.analyze_split_impact()
        print(f"åˆè¨ˆ ã‚µã‚¤ã‚º: {impact['total_size_bytes']:,} bytes")
        print(f"  Core: {impact['core_size_bytes']:,} bytes ({impact['core_percentage']:.1f}%)")
        print(f"  Feature: {impact['feature_size_bytes']:,} bytes")
        print()
        print(f"ãƒˆãƒ¼ã‚¯ãƒ³ ã‚³ã‚¹ãƒˆ:")
        print(f"  åˆ†å‰²å‰: {impact['estimated_tokens_before']:,} ãƒˆãƒ¼ã‚¯ãƒ³/ã‚»ãƒƒã‚·ãƒ§ãƒ³")
        print(f"  åˆ†å‰²å¾Œ: {impact['estimated_tokens_after']:,} ãƒˆãƒ¼ã‚¯ãƒ³/ã‚»ãƒƒã‚·ãƒ§ãƒ³ (coreã®ã¿)")
        print(f"  å‰Šæ¸›: {impact['token_savings']:,} ãƒˆãƒ¼ã‚¯ãƒ³ ({impact['savings_percentage']:.1f}%)")

    elif args.split:
        if args.dry_run:
            print("\nğŸ” Dry Run ãƒ¢ãƒ¼ãƒ‰ (å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆãªã—)")
        else:
            print("\nâœ‚ï¸  Wisdom åˆ†å‰² é–‹å§‹...")

        result = splitter.split_patterns(dry_run=args.dry_run)
        print("=" * 60)
        print(f"åŸæœ¬ ãƒ•ã‚¡ã‚¤ãƒ«: {result['source_file']}")
        print(f"åˆè¨ˆ ã‚»ã‚¯ã‚·ãƒ§ãƒ³: {result['total_sections']}")
        print(f"  Core: {result['core_sections']} ã‚»ã‚¯ã‚·ãƒ§ãƒ³ (â‰¥{result['core_threshold']}å› å‚ç…§)")
        print(f"  Feature: {result['feature_sections']} ã‚»ã‚¯ã‚·ãƒ§ãƒ³")

        if not args.dry_run:
            print(f"\nãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {result['backup_file']}")
            print("\nâœ… åˆ†å‰² å®Œäº†")
            print("   core-patterns.md â†’ å…¨ã¦ã® ã‚»ãƒƒã‚·ãƒ§ãƒ³ã§ãƒ­ãƒ¼ãƒ‰")
            print("   feature-patterns.md â†’ å¿…è¦ æ™‚ å‚ç…§")

    elif args.merge:
        print("\nğŸ”€ Wisdom ãƒãƒ¼ã‚¸ é–‹å§‹...")
        splitter.merge_patterns()

    else:
        parser.print_help()


if __name__ == "__main__":
    main()
